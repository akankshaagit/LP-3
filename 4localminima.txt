Implement Gradient Descent Algorithm to
find the local minima of a function

from sympy import Symbol, lambdify
import matplotlib.pyplot as plt
import numpy as np

x = Symbol('x')

def gradient_descent(
    function, start, learn_rate, n_iter=10000, tolerance=1e-06, step_size=1
):
    gradient = lambdify(x, function.diff(x))
    function = lambdify(x, function)
    points = [start]
    iters = 0                           #iteration counter
    
    while step_size > tolerance and iters < n_iter:
        prev_x = start                  #Store current x value in prev_x
        start = start - learn_rate * gradient(prev_x) #Grad descent
        step_size = abs(start - prev_x) #Change in x
        iters = iters+1                 #iteration count
        points.append(start)
    print("The local minimum occurs at", start)
    
    # Create plotting array
    x_ = np.linspace(-7,5,100)
    y = function(x_)

    # setting the axes at the centre
    fig = plt.figure(figsize = (10, 10))
    ax = fig.add_subplot(1, 1, 1)
    ax.spines['left'].set_position('center')
    ax.spines['bottom'].set_position('zero')
    ax.spines['right'].set_color('none')
    ax.spines['top'].set_color('none')
    ax.xaxis.set_ticks_position('bottom')
    ax.yaxis.set_ticks_position('left')

    # plot the function
    plt.plot(x_,y, 'r')
    plt.plot(points, function(np.array(points)), '-o')

    # show the plot
    plt.show()


function=(x+5)**2

gradient_descent(
     function=function, start=3.0, learn_rate=0.2, n_iter=50
)

current_x = 2
rate = 0.01 # Learning rate
precision = 0.000001  # This tells us when to stop the algorithm
delta_x = 1
max_iterations = 10000 # Maximum number of iterations
iteration_counter = 0

# dy/dx of eqn = 2*(x+3)
def slope(x):
    return 2*(x+3)

def value_y(x):
    return (x+3)**2
y = []
x = []
y.append(value_y(current_x))
x.append(current_x)

while delta_x > precision and iteration_counter < max_iterations:
    previous_x = current_x
    current_x = previous_x - rate * slope(previous_x)
    y.append(value_y(current_x))
    x.append(current_x)
    delta_x = abs(previous_x - current_x)
    print(f"Iteration {iteration_counter+1}")
    iteration_counter += 1
    print(f"X = {current_x}")

print(f"Local Minima occurs at: {current_x}")
    


=====================================================================================

What is Gradient Descent?

Answer: Gradient Descent is an optimization algorithm used to find the local minimum of a function by iteratively moving in the direction of the negative gradient.
What does the learn_rate parameter do?

Answer: It controls the step size of each iteration in gradient descent. A higher rate moves faster but may overshoot; a lower rate moves slower but is more stable.
How does the algorithm know when to stop?

Answer: It stops when the change in x (step_size) is less than tolerance or the maximum number of iterations (n_iter) is reached.
What is the role of lambdify in this code?

Answer: lambdify converts a symbolic expression (like the function or its derivative) into a callable Python function that can accept numerical input.
Why do we use the derivative in Gradient Descent?

Answer: The derivative gives the slope of the function, which tells us the direction to move x to decrease the function‚Äôs value.
Explain the purpose of while step_size > tolerance in the code.

Answer: This condition ensures the loop runs until the change in x becomes very small, indicating convergence to a local minimum.
What happens if the learning rate is too high?

Answer: If the learning rate is too high, the algorithm might overshoot the minimum, causing it to diverge.
How is the gradient descent path visualized?

Answer: matplotlib is used to plot the function, and the points generated by each iteration are plotted as markers to show the path of gradient descent.
What is the output of the code, and what does it represent?

Answer: The code prints the local minimum and displays a plot of the function with the gradient descent path, showing how x converges to the minimum.
What is the role of tolerance in the code?

Answer: tolerance is the threshold for the minimum change in x to continue iterations. It ensures that gradient descent stops when it‚Äôs close enough to the minimum.

In the context of Gradient Descent, the algorithm iteratively adjusts 
ùë•
x to move towards a local minimum by following the direction of the negative gradient, which indicates where the function decreases.A local minimum is a point in a function where the function value is lower than at any nearby points. In other words, it's a "valley" in the graph of the function, although it may not be the lowest point (or global minimum) of the entire function.













